<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and Localization in Diffusion Era</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and Localization in Diffusion Era</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="www.linkedin.com/in/chenayng-z-594bb3264" target="_blank">Chenyang Zhu</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/xing-zhang-82306a270/?originalSubdomain=jp" target="_blank">Xing Zhang</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yuyang Sun</a><sup>1,2</sup>,</span>
                    <span class="author-block">
                      <a href="https://cn.linkedin.com/in/ching-chun-chang-253478231" target="_blank">Ching-Chun Chang</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="https://research.nii.ac.jp/~iechizen/official/members_echizen-e.html" target="_blank">Isao Echizen</a><sup>1,2</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>The University of Tokyo<br><sup>2</sup>National Institute of Informatics, Japan</span>
                  </div>

                  
                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="./AnimeDL2M_For_Arxiv.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/FlyTweety/AnimeDL2M" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Dataset & Code</span>
                  </a>
                </span>

                <!--
                 ArXiv abstract Link 
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span> -->

            
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in image generation, particularly diffusion models, have significantly lowered the barrier for creating sophisticated forgeries, making image manipulation detection and localization (IMDL) increasingly challenging. While prior work in IMDL has focused largely on natural images, the anime domain remains underexplored—despite its growing vulnerability to AI-generated forgeries. Misrepresentations of AI-generated images as hand-drawn artwork, copyright violations, and inappropriate content modifications pose serious threats to the anime community and industry. To address this gap, we propose AnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive annotations. It comprises over two million images including real, partially manipulated, and fully AI-generated samples. 
          </p>
          <p>
            Experiments indicate that models trained on existing IMDL datasets of natural images perform poorly when applied to anime images, highlighting a clear domain gap between anime and natural images. To better handle IMDL tasks in anime domain, we further propose  AniXplore, a novel model tailored to the visual characteristics of anime imagery. Extensive evaluations demonstrate that AniXplore achieves superior performance compared to existing methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- 数据集图片 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title is-3">AnimeDL-2M Dataset</h2>
      <!-- 图片 -->
      <img src="static/images/pipeline.png"  alt="IMG pipeline" style="width: 70%; height: auto;" />


    </div>
  </div>
</section>
<!-- 静态图片展示结束 -->


<!-- 数据集介绍 -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            The dataset is constructed based on the above pipeline. For each input image, we use 3 inpainting methods and 3 text-to-image methods to generate 3 partially manipulated images and 3 entirely fake images.
          </p>
          <p>
            For each group of original, edited, or generated images, AnimeDL-2M not only provides segmentation masks as in traditional datasets, but also includes additional annotations such as image captions, object descriptions, mask labels, and editing methods. These enriched annotations enable a broader range of tasks to be conducted on this dataset and are intended to facilitate future research in related domains.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- 介绍统计图 
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-vcentered">


      <div class="column is-half has-text-centered">
        <img src="static/images/Tab_dataset_staistics.png" alt="Descriptive alt text" style="width: 50%; height: auto;" />
      </div>

      <div class="column is-half">
        <h2 class="title is-5">Dataset Statics</h2>
        <p class="content has-text-justified">
          AnimeDL-2M contains 639,268 real images, 779,502 partially fake images and 884,129 fully AI-generated images. The CivitAI subset contains images generated by 14 different base models.
        </p>
      </div>

    </div>
  </div>
</section> -->



<!-- 图文轮播 介绍数据集-->
<section class="section">
  <div class="container is-max-desktop">
    <div id="image-text-carousel" class="carousel" data-autoplay="true" data-delay="5000">
      
      <!-- 第 1 项 -->
      <div class="carousel-item">
        <div class="columns is-vcentered">
          <div class="column is-half has-text-centered">
            <img src="static/images/Tab_dataset_staistics.png" alt="Descriptive alt text" style="width: 50%; height: auto;" />
          </div>
          <div class="column is-half">
            <h2 class="title is-5">Dataset Statics</h2>
            <p class="content has-text-justified">
              AnimeDL-2M contains 639,268 real images, 779,502 partially fake images and 884,129 fully AI-generated images. The CivitAI subset contains images generated by 14 different base models.
            </p>
          </div>
        </div>
      </div>

      <!-- 第 2 项 -->
      <div class="carousel-item">
        <div class="columns is-vcentered">
          <div class="column is-half has-text-centered">
            <img src="static/images/Fig_aesthetic_score.png" alt="Image 2" style="width: 80%; height: auto;" />
          </div>
          <div class="column is-half">
            <h2 class="title is-5">Aesthetic Score</h2>
            <p class="content has-text-justified">
              Images in AnimeDL-2M achieve high aesthetic scores by the latest evaluation model
              <a href="https://arxiv.org/abs/2405.14705" target="_blank">MPS</a>.
            </p>
          </div>
        </div>
      </div>

      <!-- 第 3 项 -->
      <div class="carousel-item">
        <div class="columns is-vcentered">
          <div class="column is-half has-text-centered">
            <img src="static/images/Fig_subject_distribution.png" alt="Image 3" style="width: 80%; height: auto;" />
          </div>
          <div class="column is-half">
            <h2 class="title is-5">Subjects Distribution</h2>
            <p class="content has-text-justified">
              The distribution of manipulated subjects is fairly diverse and seemingly, which contributes to model generalization and enables a more comprehensive evaluation of model performance.
            </p>
          </div>
        </div>
      </div>

    </div>
  </div>
</section>






<!-- 方法图片 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title is-3">AniXplore Model</h2>
      <!-- 图片 -->
      <img src="static/images/method.png" alt="IMG METHOD" style="width: 70%; height: auto;" />
    </div>
  </div>
</section>
<!-- 静态图片展示结束 -->


<!-- 方法介绍 -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Anime images exhibit distinctive visual characteristics that distinguish them from natural daily images, such as unrealistic lighting conditions, geometric abstractions, and the absence of sensor noise. These distinct properties underscore the necessity for specialized methods tailored to the IMDL tasks in the anime domain.
          </p>
          <p>
            While it is commonly assumed that anime images contain fewer high-frequency components such as complex textures or stochastic noise, an overlooked yet crucial aspect is their retention of edge information in mid-to-high frequencies, especially the line contours. As anime images typically have clean and uncluttered scenes, line work in these images is generally sharp and well-defined. Furthermore, as these lines are manually drawn, they tend to exhibit a consistent artistic style across the image. Consequently, localized inconsistencies in stroke thickness, color, or drawing style may serve as effective cues for identifying image manipulations.
          </p>
          <p>
            Additionally, prior studies have demonstrated that image manipulations frequently occur at the object level. Anime images, which typically comprise a limited number of semantically salient objects with well-defined boundaries, are especially amenable to object-level semantic reasoning. Motivated by these insights, we propose  AniXplore, a model with dual-branch architecture that integrates semantic representations with frequency-aware features to enhance the IMDL in AI-generated anime images.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- 实验1图片 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title is-3">Zero-Shot and Fine-Tuned Performance</h2>
      <!-- 图片 -->
      <div class="is-flex is-justify-content-center is-align-items-center" style="gap: 20px;">
        <img src="static/images/tab_zeroshot.png" alt="Left image" style="width: 30%; height: auto;" />
        <img src="static/images/tab_finetune.png" alt="Right image" style="width: 30%; height: auto;" />
      </div>
    </div>
  </div>
</section>
<!-- 静态图片展示结束 -->

<!-- 方法介绍 -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Models trained on conventional IMDL datasets lack generalizability to the distribution of AI-edited anime images.  Models pre-trained on the GRE dataset perform relatively better. This implies that certain features of AI-generated manipulations can be learned and partially transferred. However, these models still perform poorly.
          </p>
          <p>
            After fine-tuning on AnimeDL-2M, all models show significant improvements, confirming both the high training value and the annotation quality of the dataset. Some models achieve surprisingly high F1 scores in detection task. This suggests that while models cannot precisely locate manipulated regions, they can still capture global statistical cues such as unnatural frequency artifacts or noise distribution that distinguish fake images from real ones at a coarse level.
          </p>
          <p>
            These findings collectively validate the presence of substantial domain gaps across manipulation methods and image styles, especially for localization tasks. Therefore, AnimeDL-2M serves as a necessary contribution to bridge this gap, offering a dedicated benchmark for AI-edited anime image forensics.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- 实验2图片 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title is-3">Cross-Dataset and In-the-Wild Evaluation</h2>
      <!-- 图片 -->
      <div class="is-flex is-justify-content-center is-align-items-center" style="gap: 20px;">
        <img src="static/images/tab_cross.png" alt="Left image" style="width: 50%; height: auto;" />
        <img src="static/images/tab_civitai.png" alt="Right image" style="width: 30%; height: auto;" />
      </div>
    </div>
  </div>
</section>
<!-- 静态图片展示结束 -->

<!-- 方法介绍 -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            All models exhibit generally poor performance on the localization task under cross-dataset settings. This also suggests that training or fine-tuning on the target domain can substantially improve localization performance.
          </p>
          <p>
            Generalize ability on detection task does not strongly correlate with localization performance. Some models achieve high detection accuracy across domains despite limited localization ability. This implies that detection generalization may depend more on the robustness of the model architecture than on the ability to capture specific forgery artifacts. 
          </p>
          <p>
            PSCC, as the only model that uses RGB images as the sole input modality, demonstrates the weakest generalization, highlighting the importance of multi-modal or multi-channel feature inputs for generalizability.
          </p>
          <p>
            Meanwhile, both TruFor and MMFusion incorporate noise-based features, yet their performance differs significantly. This suggests that not all handcrafted features are equally effective, and the design of the feature extractor plays a critical role in mitigating overfitting. Therefore, careful selection and design of input features is essential for building more generalizable forensic models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!--BibTex citation 
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
